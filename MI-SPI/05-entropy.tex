\section{Přednáška 5 -- Entropie}

\begin{description}
    \item[Entropie] Míra neuspořádanosti
\end{description}

Entropie $H(X)$ dikrétní náhodné veličiny:

$$
    H(X) = - \sum_{x\in X}{p(x) \log{p(x)}}
$$

Logaritmus má základ 2, navíc je potřeba dodefinovat, že $0\log{0} = 0$.
Entropie je invariantní vůči libovolné transformaci hodnotu, která ponechá jejich pravděpodobnosti (číselné hodnoty nehrajou v entropii roli).

Entropie se uvádí v bitech.

Lze ji chápat jako střední hodnotu
$$
    H(x) = -E\log{p(X)} = EI(X)
$$

kde 

$$
    I(x) = -\log{p(x)}
$$

pro všechna $x \in X$. $I(X)$ je vlastní informace (míra neurčitosti) hodnoty $x$.
Pro entropii platí, že $H(X) \geq 0$.

\subsection{Sdružená entropie}

$$
    H(X, Y) = -\sum_{x\in X}{} \sum_{y\in Y}{p(x,y)\log{p(x,y)}}
$$

\subsection{Podmíněná entropie}

$$
    H(X|Y) = -\sum_{x\in X}{} \sum_{y\in Y}{p(x,y)\log{p(x|y)}}
$$

\subsection{Řetězové pravidlo}

$$
    H(X.Y) = H(X) + H(Y|X)
$$

\subsection{Vzájemná informace}

$$
    I(X; Y) = \sum_{x\in X}{} \sum_{y\in Y}{p(x,y)\log{\frac{p(x,y)}{p(x)p(y)}}}
$$

\img{05-entropy-x-information.png}{Vstah vzájemné informace a entropie}
